{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score, f1_score, make_scorer,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('clean/train.csv')\n",
    "data.fillna(np.nan, inplace=True)\n",
    "data.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"TARGET\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = np.sum(data.isnull(),axis=0)/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the feature in which the percentage of missing value exceed 20%\n",
    "# drop 163\n",
    "drop_col = list(missing[missing>0.2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop = data.drop(drop_col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = np.sum(data_drop.isnull(),axis=0)/data_drop.shape[0]\n",
    "missing_col = list(missing[missing>0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_col = []\n",
    "continuous_col = []\n",
    "for col in missing_col:\n",
    "    if sorted(list(data_drop[col].unique())) == [0,1]:\n",
    "        binary_col.append(col)\n",
    "    else:\n",
    "        continuous_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(binary_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all missing are continuous feature, imputing with median\n",
    "fill_na = lambda col:col.fillna(col.median())\n",
    "data_drop[continuous_col] = data_drop[continuous_col].apply(fill_na, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_drop['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use all features we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_origin = data_drop[list(data_drop.columns[2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_origin, x_test_origin, y_train_origin, y_test_origin = train_test_split(X_origin,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dimension reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each column to (0,1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler()\n",
    "scale.fit(X_origin)\n",
    "X_scale = scale.transform(X_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = pca.explained_variance_ratio_\n",
    "for n_components in range(0,X_scale.shape[1]):\n",
    "    total = np.sum(var[:n_components])\n",
    "    if total >= 0.99:\n",
    "        print(n_components)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sqrt(pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = X_pca[:,:n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Oversampling: SMOTE with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# after resampling # minority / # majority = 4/6\n",
    "oversample = SMOTE(sampling_strategy=4/6, k_neighbors=5, random_state=0)\n",
    "x_train_origin_os, y_train_origin_os = oversample.fit_resample(x_train_origin, y_train_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Oversampling: SMOTE with PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy=4/6, k_neighbors=5, random_state=0)\n",
    "x_train_pca_os, y_train_pca_os = oversample.fit_resample(x_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling  \n",
    "\n",
    "Four versions of (X,Y):  \n",
    "1) x_train_origin, y_train_origin  \n",
    "2) x_train_pca, y_train_pca  \n",
    "3) x_train_origin_os, y_train_origin_os  \n",
    "4) x_train_pca_os, y_train_pca_os  \n",
    "\n",
    "Use 3-fold CV (GridSearchCV) to tune hyperparameters  \n",
    "\n",
    "Metircs: auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Parameter Tuning Codes\n",
    "# Hyper parameter tuning, input model, output best set of parameters\n",
    "def tuning(x1_data, y, model_name):\n",
    "    assert model_name in ['Random Forest','Decision Tree','Naive Bayes','Logistic']\n",
    "    \n",
    "    if model_name ==\"Decision Tree\":\n",
    "        max_depth = [3,5,10]\n",
    "        min_samples_split = [2,4,6]\n",
    "        min_samples_leaf = [1,2,4] \n",
    "        class_weight=[{1:1}, {1:6},{2:1}]\n",
    "        hyperparameters = dict(max_depth = max_depth,min_samples_split = min_samples_split, \n",
    "                               min_samples_leaf = min_samples_leaf,class_weight=class_weight)\n",
    "        dt=DecisionTreeClassifier(random_state=0)\n",
    "        grid = GridSearchCV(dt, hyperparameters, scoring=make_scorer(roc_auc_score, needs_proba=True),cv=3,n_jobs=-1,verbose=10)\n",
    "        best_param = grid.fit(x1_data, y)\n",
    "        return best_param\n",
    "    \n",
    "    elif model_name =='Random Forest':\n",
    "        n_estimators = [10,50,100]\n",
    "        max_depth = [3,5,20]\n",
    "        min_samples_split = [2, 4, 6]\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        class_weight=[{1:1},{1:6},{2:1}]\n",
    "        hyperparameters = dict(n_estimators = n_estimators, max_depth = max_depth,\n",
    "                               min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf,class_weight=class_weight)\n",
    "        rf=RandomForestClassifier(random_state=0)\n",
    "        grid = GridSearchCV(rf, hyperparameters, scoring=make_scorer(roc_auc_score, needs_proba=True),cv=3,n_jobs=-1,verbose=10)\n",
    "        best_param = grid.fit(x1_data, y)\n",
    "        return best_param\n",
    "    \n",
    "    elif model_name ==\"Naive Bayes\":\n",
    "        hyperparameters = dict()\n",
    "        NB=GaussianNB()\n",
    "        grid = GridSearchCV(NB,hyperparameters,cv=5,n_jobs=-1,verbose=10)\n",
    "        best_param = grid.fit(x1_data,y)\n",
    "        return best_param\n",
    "    \n",
    "    else:\n",
    "        class_weight=[{2:1},{1:1},{1:4},{1:6}]\n",
    "        c = np.linspace(0.01, 10, 5)\n",
    "        penalty=['l1','l2']\n",
    "        hyperparameters = dict(C=c, class_weight=class_weight, penalty=penalty)\n",
    "        lr=LogisticRegression(solver='liblinear',random_state=0)\n",
    "        grid = GridSearchCV(lr, hyperparameters, scoring=make_scorer(roc_auc_score, needs_proba=True),cv=3,n_jobs=-1,verbose=10)\n",
    "        best_param = grid.fit(x1_data, y)\n",
    "        return best_param\n",
    "\n",
    "                          \n",
    "                          \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing scores, confusion matrix\n",
    "def Testing(y_test,y_pred,y_pred_pr):\n",
    "    print(f1_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(roc_auc_score(y_test,y_pred_pr))\n",
    "    \n",
    "#print result, draw bar graph for feature importance for non-pca method\n",
    "def print_result(model,x_test):\n",
    "    print(model)\n",
    "    feat_importances = pd.Series(model.best_estimator_.feature_importances_, index=x_test.columns)\n",
    "    feat_importances.nlargest(15).plot(kind='barh')\n",
    "    return pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt=tuning(x_train_origin,y_train_origin,\"Decision Tree\")\n",
    "y_dt_1_pr=dt.predict_proba(x_test_origin)[:,1]\n",
    "y_dt_1=dt.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_dt_1,y_dt_1_pr)\n",
    "cv_dt=print_result(dt,x_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2=tuning(x_train_pca,y_train_pca,\"Decision Tree\")\n",
    "y_dt_2_pr=dt2.predict_proba(x_test_pca)[:,1]\n",
    "y_dt_2=dt2.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_dt_2,y_dt_2_pr)\n",
    "cv_dt2=pd.DataFrame(dt2.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt3=tuning(x_train_origin_os,y_train_origin_os,\"Decision Tree\")\n",
    "y_dt_3_pr=dt3.predict_proba(x_test_origin)[:,1]\n",
    "y_dt_3=dt3.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_dt_3,y_dt_3_pr)\n",
    "cv_dt3=print_result(dt3,x_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4=tuning(x_train_pca_os,y_train_pca_os,\"Decision Tree\")\n",
    "y_dt_4_pr=dt4.predict_proba(x_test_pca)[:,1]\n",
    "y_dt_4=dt4.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_dt_4,y_dt_4_pr)\n",
    "cv_dt4=pd.DataFrame(dt4.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=tuning(x_train_origin,y_train_origin,\"Naive Bayes\")\n",
    "y_nb_1_pr=nb.predict_proba(x_test_origin)[:,1]\n",
    "y_nb_1=nb.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_nb_1,y_nb_1_pr)\n",
    "cv_nb=pd.DataFrame(nb.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2=tuning(x_train_pca,y_train_pca,\"Naive Bayes\")\n",
    "y_nb_2_pr=nb2.predict_proba(x_test_pca)[:,1]\n",
    "y_nb_2=nb2.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_nb_2,y_nb_2_pr)\n",
    "cv_nb2=pd.DataFrame(nb2.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb3=tuning(x_train_origin_os,y_train_origin_os,\"Naive Bayes\")\n",
    "y_nb_3_pr=nb3.predict_proba(x_test_origin)[:,1]\n",
    "y_nb_3=nb3.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_nb_3,y_nb_3_pr)\n",
    "cv_nb3=pd.DataFrame(nb3.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb4=tuning(x_train_pca_os,y_train_pca_os,\"Naive Bayes\")\n",
    "y_nb_4_pr=nb4.predict_proba(x_test_pca)[:,1]\n",
    "y_nb_4=nb4.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_nb_4,y_nb_4_pr)\n",
    "cv_nb4=pd.DataFrame(nb4.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=tuning(x_train_origin,y_train_origin,\"Random Forest\")\n",
    "y_rf_1_pr=rf.predict_proba(x_test_origin)[:,1]\n",
    "y_rf_1=rf.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_rf_1,y_rf_1_pr)\n",
    "cv_rf=print_result(rf,x_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2=tuning(x_train_pca,y_train_pca,\"Random Forest\")\n",
    "y_rf_2_pr=rf2.predict_proba(x_test_pca)[:,1]\n",
    "y_rf_2=rf2.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_rf_2,y_rf_2_pr)\n",
    "cv_rf2=pd.DataFrame(rf2.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf3=tuning(x_train_origin_os,y_train_origin_os,\"Random Forest\")\n",
    "y_rf_3_pr=rf3.predict_proba(x_test_origin)[:,1]\n",
    "y_rf_3=rf3.predict(x_test_origin)\n",
    "Testing(y_test_origin,y_rf_3,y_rf_3_pr)\n",
    "cv_rf3=print_result(rf3,x_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf4=tuning(x_train_pca_os,y_train_pca_os,\"Random Forest\")\n",
    "y_rf_4_pr=rf4.predict_proba(x_test_pca)[:,1]\n",
    "y_rf_4=rf4.predict(x_test_pca)\n",
    "Testing(y_test_pca,y_rf_4,y_rf_4_pr)\n",
    "cv_rf4=pd.DataFrame(rf4.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data for Logistic Regression\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "scaler.fit(x_train_origin)\n",
    "x_train_1 =  scaler.transform(x_train_origin)\n",
    "x_test_1 = scaler.transform(x_test_origin)\n",
    "x_train_2 =  scaler.transform(x_train_origin_os)\n",
    "x_test_2 = scaler.transform(x_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logit_1 = tuning(x_train_origin, y_train_origin, 'Logistic')\n",
    "display(clf_logit_1)\n",
    "pred_logit_1 = clf_logit_1.predict(x_test_origin)\n",
    "pred_logit_1_pr=clf_logit_1.predict_proba(x_test_1)\n",
    "Testing(y_test_origin, pred_logit_1, pred_logit_1_pr[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logit_2 = tuning(x_train_pca, y_train_pca, 'Logistic')\n",
    "display(clf_logit_2)\n",
    "pred_logit_2 = clf_logit_2.predict(x_test_pca)\n",
    "pred_logit_2_pr=clf_logit_2.predict_proba(x_test_pca)\n",
    "Testing(y_test_pca, pred_logit_2, pred_logit_2_pr[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logit_3 = tuning(x_train_2, y_train_origin_os, 'Logistic')\n",
    "display(clf_logit_3)\n",
    "pred_logit_3 = clf_logit_3.predict(x_test_2)\n",
    "pred_logit_3_pr=clf_logit_3.predict_proba(x_test_2)\n",
    "Testing(y_test_origin, pred_logit_3, pred_logit_3_pr[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logit_4 = tuning(x_train_pca_os, y_train_pca_os, 'Logistic')\n",
    "display(clf_logit_4)\n",
    "pred_logit_4 = clf_logit_4.predict(x_test_pca)\n",
    "pred_logit_4_pr=clf_logit_4.predict_proba(x_test_pca)\n",
    "Testing(y_test_pca, pred_logit_4, pred_logit_4_pr[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural=MLPClassifier(solver='adam',hidden_layer_sizes=(10,3,3), random_state=0,max_iter=100,verbose=10)\n",
    "nu=neural.fit(x_train_pca,y_train_pca)\n",
    "y_pred_neural_pr=nu.predict_proba(x_test_pca)[:,1]\n",
    "y_pred_neural=nu.predict(x_test_pca)\n",
    "print(f1_score(y_test_pca,y_pred_neural))\n",
    "print(confusion_matrix(y_test_pca,y_pred_neural))\n",
    "print(roc_auc_score(y_test_pca,y_pred_neural_pr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
